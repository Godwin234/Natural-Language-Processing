{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Based Distributional Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will implement a simple count based model. We will not try to optimize the model in order to get the best results possible. Instead we will try to keep things as simple as possible. Thus the main principles can become clear. \n",
    "\n",
    "We will use a mid-sized corpus as a compromise between fast processing and usefull results. \n",
    "\n",
    "In this notebook we will use German data. However, not much knowledge of German is required. And learning some new German words on the fly is not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Caution</span>**\n",
    "\n",
    "Some of the cells require quite long computation time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use a corpus of 300k sentences from wikipedia collected by the university of Leipzig. You can download the required data from http://wortschatz.uni-leipzig.de/en/download/ . The file used here is **deu_wikipedia_2016_300K-sentences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einlesen der Daten\n",
    "\n",
    "The texts in the corpus are already split into sentences. We read thes sentence and word-tokenize each sentence. To save time we use the infected words and do not do anly lemmatization or stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "sentences = []\n",
    "\n",
    "#Caution: change the path to this file!\n",
    "quelle = codecs.open('../../TMNB/Corpora/deu_wikipedia_2016_300K/deu_wikipedia_2016_300K-sentences.txt','r','utf8')\n",
    "for line in quelle:\n",
    "    nr,satz = line.split('\\t')\n",
    "    sentences.append(nltk.word_tokenize(satz.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether the senteces are in the list as we expect them to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1819',\n",
       " 'wurde',\n",
       " 'daher',\n",
       " 'ein',\n",
       " 'neues',\n",
       " '„',\n",
       " 'Vereidigungsbuch',\n",
       " '“',\n",
       " 'angelegt',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[447]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors of Co-occurrence Values\n",
    "\n",
    "\n",
    "We write a function that computes vectors with co-occurrence numbers for a number of  words with a given list of 'context' words. As context words we take all words that exceed a specified minimum frequency. KCo-occurrence with rare words will hardly contribute to the comparison between context vectors. Another parameter that needs to be set is the maximum distance between two words to count as co-competition, known as the window size. We proceed sentence by sentence here. This means that the last word of a sentence and the first word of the next sentence are never counted as co-occurring. Note, however, that sentence bondaries are often not taken into account in such procedures. It is not clear whether this has a seriosu impact. Another detail to note is that we first calculate the window size for the competition, and then determine the relevant words in the window. Alternatively, you can first remove all irrelevant words and then determine the words within the window.\n",
    "\n",
    "In general, a larger window can compensate for a too small corpus. In addition, a smaller window captures more syntactical properties of a word, while a larger window takes into account broader semantic relationships.\n",
    "\n",
    "When all co-occurrence values are calculated, we normalize the length of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b6a93573d1c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def mag(x):\n",
    "    return np.sqrt(x.dot(x))\n",
    "\n",
    "\n",
    "def makeCV(words,sentences, window = 2, minfreq = 10):\n",
    "    #first count all words\n",
    "    freq = Counter()\n",
    "    for s in sentences:\n",
    "        freq.update(s)\n",
    "        \n",
    "    #determine which words have to be used as features or context words\n",
    "    context_words = [w for w,f in freq.items() if f > minfreq]\n",
    "    \n",
    "    #we add all words to the context words, if they are not already in that list. Just for convenience to have all words in one list. \n",
    "    for w in words:\n",
    "        if w not in context_words:\n",
    "            context_words.append(w)\n",
    "    dim = len(context_words)\n",
    "    \n",
    "    #Give each context word a unique number and build dictionaries to switch between numbers and words\n",
    "    cw2nr = {}\n",
    "    for i in range(dim):\n",
    "        cw = context_words[i]\n",
    "        cw2nr[cw] = i\n",
    "              \n",
    "    w2nr = {}\n",
    "    for i in range(len(words)):\n",
    "        w = words[i]\n",
    "        w2nr[w] = i\n",
    "    \n",
    "    \n",
    "    #initialize a matrix\n",
    "    #We use  a sparse matrix class from scipy\n",
    "    matrix = sparse.lil_matrix((len(words),dim))\n",
    "    \n",
    "    #Now we start the real work. We iterate through all sentences and cont the co-occurrences!\n",
    "    n = 0\n",
    "    for s in sentences:\n",
    "        n+=1\n",
    "        i_s = [cw2nr.get(w,-1) for w in s]\n",
    "        for i in range(len(s)):\n",
    "            w = s[i]\n",
    "            if w in words:\n",
    "                i_w = w2nr[w]\n",
    "                for j in range(max(0,i-window),min(i+window+1,len(s))):\n",
    "                    if i != j: # a word is not in its own context!\n",
    "                        i_cw = i_s[j]\n",
    "                        if i_cw > 0:\n",
    "                            matrix[i_w,i_cw] += 1\n",
    "          \n",
    "    #finally make a dictionary with vectors for each word\n",
    "    wordvectors = {}\n",
    "    for w,i_w in w2nr.items():\n",
    "        v = matrix[i_w].toarray()[0]\n",
    "        wordvectors[w] = v/mag(v)\n",
    "    return wordvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the function for a short list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_count =  makeCV(['Kloster','Kirche','Garten','Haus','Hof','Schweden','Deutschland','betrachten','anschauen','beobachten'],sentences,window=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the vectors have the same length, we can use the inner product, which is identical to the cosine, for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8827709543096601\n",
      "0.702106425947288\n"
     ]
    }
   ],
   "source": [
    "print(vectors_count['Schweden'].dot(vectors_count['Deutschland']))\n",
    "print(vectors_count['Schweden'].dot(vectors_count['Hof']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to know, what words are most similar to a given word. To do so, we need to compare a word with each other word in the list. We use an ordered list to store the results. Since these list always sort ascending, we need to consider always the last elements of this list. Finallye, we return the results in inverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect \n",
    "\n",
    "def most_similar(word,vectors,n):\n",
    "    best = []\n",
    "    vec_w = vectors[word]\n",
    "    for z in vectors:\n",
    "        if z == word:\n",
    "            continue\n",
    "        sim = vec_w.dot(vectors[z])\n",
    "\n",
    "        #we have to add this result only, if we do not yet have n results, or if the similarity is larger than the similarity with the last element in the list (actually the first since we sort ascending)\n",
    "        if len(best) < n or sim > best[0][0]:\n",
    "            bisect.insort(best,(sim,z))\n",
    "            best = best[-n:]\n",
    "       \n",
    "    return best[::-1] #present the list in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8130680728909148, 'Hof'),\n",
       " (0.664617096252052, 'Haus'),\n",
       " (0.6388171089882518, 'Schweden')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Garten',vectors_count,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is more interesing to find the most similar word if we have more words to choose from. Let us collect some mid-frequency words to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12165\n"
     ]
    }
   ],
   "source": [
    "wortfrequenz = Counter()\n",
    "\n",
    "for satz in sentences:\n",
    "    wortfrequenz.update(satz)\n",
    "\n",
    "vocabulary  = [w for w,f in wortfrequenz.items() if 30 < f < 3000]\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it takes some time to compute all vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_count = makeCV(vocabulary,sentences,window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.930302724001756, 'Park'),\n",
       " (0.8713290722481782, 'Saal'),\n",
       " (0.8648406935911824, 'Tempel'),\n",
       " (0.8631281311361956, 'Bezirk'),\n",
       " (0.8620724034396585, 'Sturm'),\n",
       " (0.858317906806122, 'Keller'),\n",
       " (0.8572571532271944, 'Raum'),\n",
       " (0.8549354330124461, 'Wald'),\n",
       " (0.8549228454240734, 'Kreis'),\n",
       " (0.846418171910536, 'Sinn')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Garten',vectors_count,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9585602463943913, 'verstehen'),\n",
       " (0.9577321881251438, 'ermitteln'),\n",
       " (0.9575996519992644, 'schaffen'),\n",
       " (0.9566353415343977, 'bauen'),\n",
       " (0.9549111806749297, 'beobachten'),\n",
       " (0.9537790612894577, 'verwenden'),\n",
       " (0.9526877411161037, 'erhöhen'),\n",
       " (0.9525533613296538, 'bringen'),\n",
       " (0.9478641686123344, 'retten'),\n",
       " (0.947498120959553, 'kämpfen')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('betrachten',vectors_count,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9462175999010392, 'Ungarn'),\n",
       " (0.9379372993961502, 'Polen'),\n",
       " (0.9353772897649918, 'Frankreich'),\n",
       " (0.9310094388128083, 'Australien'),\n",
       " (0.93014008593448, 'Spanien'),\n",
       " (0.9239015030110829, 'Russland'),\n",
       " (0.9229256210189929, 'Italien'),\n",
       " (0.922348217698481, 'England'),\n",
       " (0.9189920519494904, 'Ägypten'),\n",
       " (0.9183081684907252, 'Kanada')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Schweden',vectors_count,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9482467477877202, 'Leiter'),\n",
       " (0.9404963751501492, 'Chef'),\n",
       " (0.9368719576058703, 'Kommandeur'),\n",
       " (0.9341183267714239, 'Vorsitzender'),\n",
       " (0.923324253533616, 'Präsident'),\n",
       " (0.9104529525364085, 'Mitglied'),\n",
       " (0.8922921792779437, 'Vizepräsident'),\n",
       " (0.8728248376457493, 'Vorstandsmitglied'),\n",
       " (0.8697536816828365, 'Sekretär'),\n",
       " (0.8598231918685795, 'Kommandant')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Direktor',vectors_count,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "The examples look nice, but how good are our vectors? There are a number of tests that can be done to check the quality of the vectors. One type of test is to compare the calculated similarity between two words to the similarity that subjects have given for word pairs. We can simply use the correlation to check the similarity.\n",
    "\n",
    "A dataset with similarity assessments for German word pairs is the so-called Gur350 dataset, which was developed by the working group of Prof. Dr. Iryna Gurevych at the TU Darmstadt. Further information and a link for the download can be found here: https://www.informatik.tu-darmstadt.de/ukp/research_6/data/semantic_relatedness/german_relatedness_datasets/index.en.jsp\n",
    "\n",
    "The data does not give a similarity between the words but a relationship, which is not exactly the same. For the evaluation, we only use the word pairs for which both words are contained in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "testfile = codecs.open('../../TMNB/Corpora/wortpaare350.gold.pos.txt','r','utf8')\n",
    "testfile.readline()\n",
    "\n",
    "testdata = []\n",
    "missing = set()\n",
    "for line in testfile:\n",
    "    w1,w2,sim,p1,p2 = line.split(':')\n",
    "    if w1 in vocabulary and w2 in vocabulary:\n",
    "        testdata.append((w1,w2,float(sim)))\n",
    "    \n",
    "def evaluate(data,vectors):\n",
    "    gold = []\n",
    "    predicted = []\n",
    "    for v,w,sim in data:\n",
    "        pred = vectors[v].dot(vectors[w])\n",
    "\n",
    "        gold.append(sim)\n",
    "        predicted.append(pred)\n",
    "        #print(v,w,pred,sim,sep = '\\t')\n",
    "        \n",
    "    \n",
    "    av_p = sum(predicted)/len(predicted)\n",
    "    av_g = sum(gold)/len(gold)\n",
    "    \n",
    "    cov = 0\n",
    "    var_g = 0\n",
    "    var_p = 0\n",
    "    for s,t in zip(gold,predicted):\n",
    "        cov += (s-av_g) * (t-av_p)\n",
    "        var_g += (s-av_g) * (s-av_g)\n",
    "        var_p += (t-av_p) * (t-av_p)\n",
    "        \n",
    "    return cov / math.sqrt(var_g*var_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we are using only a very small part of the test data from the Gur350 dataset for testing. Thus we cannot compare our results to official results for these data! We can of course include more words in our vocabulary of mid-frequency words, but many words simpy are not in our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16596262818469823"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(testdata,vectors_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the good looking lists that we generated above, we see that the calculated similarities correlate only very weakly with the similarity judgments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Somewhat more advanced\n",
    "\n",
    "\n",
    "Unser erster Versuch bietet noch verschiedene Optimierungsmöglichkeiten. Zunächst nutzen wir nicht die einfache Kookkurrenzhäufigkeiten sondern die _Positive Pointwise Mutual Information_ (PPMI). Die  _Pointwise Mutual Information_ zwischen zwei Wörter a und b ist im Prinzip das Verhältnis zwischen der tatsächlichen Wahrscheinlichkeit, dass sie zusammen vorkommen, und die erwartete Wahrscheinlicheit, dass sie zusammen vorkommen, wenn ihre Vorkommen unabhängig voneinander wären. Wir definieren  $pmi(a,b) = log(\\frac{p(ab)}{p(a)p(b)})$. Die ppmi(a,b) ist nun die pmi(a,b), wenn dieser Wert positiv ist, und sonst 0. Die Benutzung der ppmi beruht auf der Annahme, dass nur die Tatsache, dass Wörter häufiger als erwartet zusammen vorkommen interessant ist, während geringere Wahrscheinlichkeiten vermutlich eher auf Zufall beruhen oder jedenfalls nichts interesantes über Wortpaare aussagen.\n",
    "\n",
    "Ein Problem unseres naiven Ansatzes war, dass die Vektoren extrem lang sind. Die Vektoren brauchen nicht nur sehr viel Speicherplatz. Viele der Dimensionen enthalten auch wenig oder nur redundante Informationen. Dieses Problem kann durch die Anwendung eines gängigen Dimensionsreduktionsverfahren gelöst werden. Hierunten werden wir hierfür Singular Value Decomposition nutzen und anschließend die wichtigste 100 Dimensionen nutzen.\n",
    "\n",
    "Our first attempt still offers various possibilities for optimization. First of all we could not use the simple co-occurrence frequencies but the _Positive Pointwise Mutual Information_ (PPMI). The _Pointwise Mutual Information_ between two words a and b is in principle the ratio between the actual probability that they will occur together and the expected probability that they will occur if their occurrences were independent. We define $ pmi (a, b) = log (\\ frac {p (ab)} {p (a) p (b)}) $. The ppmi(a, b) is now the pmi(a, b) if this value is positive, and otherwise 0. The use of the ppmi is based on the assumption that only the fact that words occur together more often than expected is interesting, while lower probabilities are more likely to be based on coincidence or in any case do not say anything interesting about word pairs.\n",
    "\n",
    "Another problem with our naive approach was that the vectors are extremely long. The vectors not only need a lot of storage space. Many of the dimensions also contain little or only redundant information. This problem can be solved by using a common dimension reduction process. Below we will use Singular Value Decomposition for this and then use the most important 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def makeCV_SVD(words,sentences, window = 2, minfreq = 10, size =256):\n",
    "\n",
    "    freq = Counter()\n",
    "    for s in sentences:\n",
    "        freq.update(s)\n",
    "    \n",
    "    context_words = [w for w,f in freq.items() if f > minfreq]\n",
    "    for w in words:\n",
    "        if w not in context_words:\n",
    "            context_words.append(w)\n",
    "    dim = len(context_words)\n",
    "    cw2nr = {}\n",
    "    for i in range(dim):\n",
    "        cw = context_words[i]\n",
    "        cw2nr[cw] = i\n",
    "              \n",
    "    w2nr = {}\n",
    "    for i in range(len(words)):\n",
    "        w = words[i]\n",
    "        w2nr[w] = i\n",
    "    \n",
    "    \n",
    "    matrix = sparse.lil_matrix((len(words),dim))\n",
    "    \n",
    "    n = 0\n",
    "    for s in sentences:\n",
    "        n+=1\n",
    "        i_s = [cw2nr.get(w,-1) for w in s]\n",
    "        for i in range(len(s)):\n",
    "            w = s[i]\n",
    "            if w in words:\n",
    "                i_w = w2nr[w]\n",
    "                for j in range(max(0,i-window),min(i+window+1,len(s))):\n",
    "                    if i != j:\n",
    "                        i_cw = i_s[j]\n",
    "                        if i_cw > 0:\n",
    "                            matrix[i_w,i_cw] += 1\n",
    "     \n",
    "    #up to here nothing new\n",
    "    N = matrix.sum()\n",
    "    \n",
    "    #Let us  get the probabilities for each word:\n",
    "    freq_w = matrix.sum(axis = 1)\n",
    "    freq_w = np.array(freq_w.T)[0]\n",
    "    prob_w = np.array(freq_w) / N\n",
    "    \n",
    "    #Let us  get the probabilities for each context word:\n",
    "    freq_cw = matrix.sum(axis = 0)\n",
    "    freq_cw = np.array(freq_cw)[0]\n",
    "    prob_cw = np.array(freq_cw) / N\n",
    "     \n",
    "    (rows,cols) = matrix.nonzero() #Returns a tuple of arrays (row,col) containing the indices of the non-zero elements of the matrix.\n",
    "    for i_w,i_cw in zip(rows,cols):\n",
    "        p = matrix[i_w,i_cw]/N\n",
    "        p_w = prob_w[i_w]\n",
    "        p_cw = prob_cw[i_cw]\n",
    "        ppmi = max(0,math.log(p/(p_w * p_cw) )) \n",
    "        matrix[i_w,i_cw] = ppmi\n",
    "\n",
    "\n",
    "    #We cannot be completely sure, that for every word we found at least som positive pmi-values.\n",
    "    #The frequent neighbors of a word eventaully are not in the set of context words\n",
    "    #A row with only zeros, will cause a problem for the SVD\n",
    "    for i in range(len(words)):\n",
    "        if np.sum(matrix[i]) == 0:\n",
    "            print(\"Empty row for:\",words[i])\n",
    "            print(\"Please remove this word from the wordlist.\")\n",
    "        \n",
    "    svd = TruncatedSVD(n_components=size)\n",
    "    svd.fit(matrix)\n",
    "    matrix = svd.transform(matrix)\n",
    "    wordvectors = {}\n",
    "    for w,i_w in w2nr.items():\n",
    "        v = matrix[i_w] \n",
    "        wordvectors[w] = v/mag(v)\n",
    "    return wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_SVD = makeCV_SVD(vocabulary,sentences,window=2, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7968530631636896, 'Saal'),\n",
       " (0.7582639181295726, 'Grundstück'),\n",
       " (0.7424050132867543, 'Wohnhaus'),\n",
       " (0.7353572526125755, 'Brunnen'),\n",
       " (0.7216225510011339, 'Schlosspark'),\n",
       " (0.7198412713285981, 'Ensemble'),\n",
       " (0.7187510254743119, 'Tempel'),\n",
       " (0.7185339902689601, 'Hof'),\n",
       " (0.7173707863339396, 'Gutshof'),\n",
       " (0.712802096863283, 'Haus')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Garten',vectors_SVD,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8120450912559876, 'denken'),\n",
       " (0.7604426711698004, 'verstehen'),\n",
       " (0.7580560782124063, 'erklären'),\n",
       " (0.7558922521025, 'erinnern'),\n",
       " (0.7553404684758059, 'bezeichnen'),\n",
       " (0.7493517804300217, 'stimmen'),\n",
       " (0.745686725644376, 'wenden'),\n",
       " (0.7430857644439924, 'bewerten'),\n",
       " (0.7419376213951389, 'sprechen'),\n",
       " (0.7412548011956499, 'glauben')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('betrachten',vectors_SVD,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9090983800625678, 'Ungarn'),\n",
       " (0.8903644566846127, 'Russland'),\n",
       " (0.8723431524686711, 'Frankreich'),\n",
       " (0.8720592656829781, 'Finnland'),\n",
       " (0.8718438934345336, 'Norwegen'),\n",
       " (0.8643964845014638, 'Dänemark'),\n",
       " (0.8633917305898804, 'Polen'),\n",
       " (0.8597463453508998, 'Italien'),\n",
       " (0.8597153905605144, 'Brasilien'),\n",
       " (0.8594229471384974, 'Rumänien')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Schweden',vectors_SVD,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7875690157831058, 'Forest'),\n",
       " (0.7860475212820364, 'Bay'),\n",
       " (0.7808347290105225, 'Castle'),\n",
       " (0.7788490377496595, 'Airport'),\n",
       " (0.7613323497522909, 'Valley'),\n",
       " (0.7611110834942387, 'Manhattan'),\n",
       " (0.756354978334809, 'Bridge'),\n",
       " (0.7486762668134073, 'Avenue'),\n",
       " (0.7454323883708169, 'Street'),\n",
       " (0.7431295213984858, 'Central')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('Park',vectors_SVD,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that park is not interpreted as a synonym for garden, but rather is perceived as part of English place names. The context 'English words' has apparently become more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5595562576911485"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(testdata,vectors_SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization was apparently successful:  the correlation has become significantly larger. Bullinaria and Levy (2007), Mohammad and Hirst (2012), Bullinaria and Levy (2012) and Kiela and Clark (2014) provide overviews of the combinations of parameters, training quantities, etc. that lead to optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubenstein, H., & Goodenough, J. B. (1965). Contextual Correlates of Synonymy. _Commun. ACM_, 8(10), 627–633. \n",
    "\n",
    "Ruge, G., & Schwarz, C. (1990). Linguistically based term associations—A new semantic component for a hyperterm system. _Proceedings of 1st International ISKO-Confercence_, Darmstadt, 88-96.\n",
    "\n",
    "Crouch, C. J. (1990). An approach to the automatic construction of global thesauri. _Information Processing & Management_, 5, 629–640.\n",
    "\n",
    "Crouch, C. J., & Yang, B. (1992). Experiments in automatic statistical thesaurus construction. In _Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval_ (pp. 77-88). ACM.\n",
    "\n",
    "Grefenstette, G. (1992). Use of syntactic context to produce term association lists for text  retrieval. 89–97.\n",
    "\n",
    "Karlgren, J., & Sahlgren, M. (2001). From Words to Understanding. In _Foundations of Real-World Intelligence (S. 294–308)_. CSLI Publications.\n",
    "\n",
    "Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations from word co-occurrence statistics: A computational study. _Behavior research methods_, 39(3), 510-526.\n",
    "\n",
    "Mohammad, S. M., & Hirst, G. (2012). Distributional measures of semantic distance: A survey. _arXiv preprint_ arXiv:1203.1858.\n",
    "\n",
    "Bullinaria, J. A., & Levy, J. P. (2012). Extracting Semantic Representations from Word Co-occurrence Statistics: Stop-lists, Stemming and SVD. _Behaviour Research Methods_, 44(3), 890–907.\n",
    "\n",
    "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In _Advances in neural information processing systems_ (pp. 3111-3119).\n",
    "\n",
    "Kiela, D., & Clark, S. (2014). A Systematic Study of Semantic Vector Space Model Parameters. In _2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)_.\n",
    "\n",
    "Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving distributional similarity with lessons learned from word embeddings. _Transactions of the Association for Computational Linguistics_, 3, 211-225."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
