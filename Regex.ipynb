{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Example Code to Work with Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build a mini-corpus to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions from last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_data(texts, filename):\n",
    "    id = 0\n",
    "    while os.path.isfile(filename.with_name(filename.name + '_' + str(id)).with_suffix('.json.gz')):\n",
    "        id += 1\n",
    "    filename = filename.with_name(filename.name + '_' + str(id)).with_suffix('.json.gz')\n",
    "    with gzip.GzipFile(filename, 'w') as data_file:\n",
    "        data_file.write(json.dumps(texts).encode('utf-8'))\n",
    "    return filename\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    return json.loads(gzip.GzipFile(filename).read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip the following cell and instead read your own corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('mycorpus_0.json.gz')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "def getWikiText(title):\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            #'pageids': page_id,\n",
    "            'titles': title,\n",
    "            'prop': 'extracts',\n",
    "            'explaintext': True\n",
    "        }\n",
    "    ).json()\n",
    "    page = next(iter(response['query']['pages'].values()))\n",
    "    if page != None and 'extract' in page:\n",
    "        return page['extract']\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def getWikiPages(category):\n",
    "    titles = []\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'list': 'categorymembers',\n",
    "            'cmtitle': 'Category:'+category,\n",
    "            'cmtype':'page',\n",
    "            'format': 'json'\n",
    "        }\n",
    "    ).json()\n",
    "\n",
    "    while(response):\n",
    "        titles.extend( [t['title'] for t in response['query']['categorymembers']] )\n",
    "        if 'continue' in response:\n",
    "            cont = response['continue']['cmcontinue']\n",
    "            response = requests.get(\n",
    "                'https://en.wikipedia.org/w/api.php',\n",
    "                params={\n",
    "                    'action': 'query',\n",
    "                    'list': 'categorymembers',\n",
    "                    'cmtitle': 'Category:' + category,\n",
    "                    'cmtype': 'page',\n",
    "                    'cmcontinue': cont,\n",
    "                    'format': 'json'\n",
    "                }\n",
    "            ).json()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        if not cont:\n",
    "            response = None\n",
    "\n",
    "    return titles\n",
    "\n",
    "\n",
    "textlist = []\n",
    "h_mark = re.compile(r'==')\n",
    "pages = getWikiPages('Machine learning')\n",
    "pages.extend(getWikiPages('Applied machine learning'))\n",
    "#pages.extend(getWikiPages('Statistical natural language processing'))\n",
    "#pages.extend(getWikiPages('Machine learning algorithms'))\n",
    "#pages.extend(getWikiPages('Artificial intelligence'))\n",
    "#pages.extend(getWikiPages('Knowledge representation'))\n",
    "for title in pages:\n",
    "    text = getWikiText(title)\n",
    "    if len(text) > 200:\n",
    "        text = h_mark.sub('\\n\\n', text)\n",
    "        textlist.append(text)\n",
    "        \n",
    "output_data_file_path = Path('.') / 'mycorpus'\n",
    "save_data(textlist,output_data_file_path)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " corpus_texts = load_data('mycorpus_0.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out some regex search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to search  pattern in the corpus we iterate over all texts and in each text over all strings. Instead of iterating over lines we also could do multiline search. For thsi some flags have to be set in the regular expression functions. We will sty wit the simple example and search in texts without linebreaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t28\t411\toverfitting\n",
      "0\t116\t405\toverfitting\n",
      "25\t1\t182\tunderfitting\n",
      "25\t2\t204\toverfitting\n",
      "25\t5\t388\toverfitting\n",
      "25\t2057\t347\toverfitting\n",
      "32\t338\t14\toverfitting\n",
      "45\t1\t264\toverfitting\n",
      "45\t182\t402\toverfitting\n",
      "45\t319\t153\toverfitting\n",
      "45\t324\t110\toverfitting\n",
      "45\t328\t80\toverfitting\n",
      "45\t391\t71\toverfitting\n",
      "45\t409\t26\toverfitting\n",
      "45\t413\t39\toverfitting\n",
      "45\t415\t30\toverfitting\n",
      "45\t502\t156\toverfitting\n",
      "48\t0\t781\toverfitting\n",
      "48\t174\t286\toverfitted\n",
      "48\t749\t660\toverfitting\n",
      "65\t0\t78\toverfitting\n",
      "65\t8\t125\toverfitting\n",
      "65\t134\t203\toverfit\n",
      "65\t465\t59\toverfitting\n",
      "65\t637\t217\toverfitting\n",
      "65\t642\t426\toverfitting\n",
      "65\t644\t62\toverfitting\n",
      "77\t16\t346\toverfitting\n",
      "81\t38\t319\toverfitting\n",
      "86\t614\t344\toverfitting\n",
      "88\t34\t388\toverfitting\n",
      "90\t9\t177\toverfitting\n",
      "98\t4\t90\toverfitting\n",
      "103\t1564\t49\toverfitting\n",
      "110\t739\t65\toverfitting\n",
      "127\t51\t722\toverfit\n",
      "128\t4\t89\toverfitting\n",
      "139\t2\t1050\toverfitting\n",
      "146\t674\t35\toverfit\n",
      "158\t0\t15\toverfitting\n",
      "158\t1\t16\tunderfitting\n",
      "158\t2\t19\toverfitting\n",
      "158\t4\t18\toverfitting\n",
      "158\t5\t39\toverfitting\n",
      "158\t7\t192\toverfitting\n",
      "158\t8\t355\toverfitted\n",
      "158\t10\t138\toverfitting\n",
      "158\t13\t24\toverfitting\n",
      "158\t14\t258\toverfitting\n",
      "158\t17\t988\toverfits\n",
      "158\t19\t43\toverfit\n",
      "158\t22\t32\toverfitting\n",
      "158\t23\t19\toverfitted\n",
      "158\t24\t16\toverfitted\n",
      "158\t29\t313\toverfitting\n",
      "158\t30\t5\tunderfitted\n",
      "158\t34\t354\toverfitting\n",
      "158\t40\t9\toverfitting\n",
      "176\t1144\t73\toverfitting\n",
      "178\t0\t389\toverfitting\n",
      "178\t2\t761\toverfitting\n",
      "178\t23\t509\toverfit\n",
      "199\t697\t69\toverfitting\n",
      "199\t698\t309\toverfitting\n",
      "199\t744\t89\toverfitting\n",
      "202\t0\t211\toverfitting\n",
      "207\t3\t514\toverfitting\n",
      "207\t4\t242\toverfitting\n",
      "207\t6\t225\toverfit\n",
      "207\t8\t229\toverfitting\n",
      "207\t12\t18\toverfitting\n",
      "207\t14\t656\toverfitting\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for text_nr in range(len(corpus_texts)):\n",
    "    text = corpus_texts[text_nr]\n",
    "    lines = re.split('[\\n\\r]+',text)\n",
    "    for line_nr in range(len(lines)):\n",
    "        line = lines[line_nr]\n",
    "        match = re.search('(over|under)fit\\w*',line)\n",
    "        if match:\n",
    "             print(text_nr,line_nr,match.start(), match.group(0),sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still might miss some matches, since we only find the first occurrence on each line! So if we have a match we have to continue searching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t28\t411\toverfitting\n",
      "0\t116\t405\toverfitting\n",
      "25\t1\t182\tunderfitting\n",
      "25\t2\t204\toverfitting\n",
      "25\t5\t388\toverfitting\n",
      "25\t5\t535\toverfit\n",
      "25\t5\t551\tunderfit\n",
      "25\t2057\t347\toverfitting\n",
      "25\t2057\t476\toverfitting\n",
      "32\t338\t14\toverfitting\n",
      "45\t1\t264\toverfitting\n",
      "45\t182\t402\toverfitting\n",
      "45\t319\t153\toverfitting\n",
      "45\t324\t110\toverfitting\n",
      "45\t328\t80\toverfitting\n",
      "45\t328\t114\toverfitting\n",
      "45\t391\t71\toverfitting\n",
      "45\t409\t26\toverfitting\n",
      "45\t409\t183\toverfitting\n",
      "45\t413\t39\toverfitting\n",
      "45\t413\t102\toverfitting\n",
      "45\t415\t30\toverfitting\n",
      "45\t415\t446\toverfitting\n",
      "45\t502\t156\toverfitting\n",
      "48\t0\t781\toverfitting\n",
      "48\t174\t286\toverfitted\n",
      "48\t749\t660\toverfitting\n",
      "65\t0\t78\toverfitting\n",
      "65\t8\t125\toverfitting\n",
      "65\t134\t203\toverfit\n",
      "65\t465\t59\toverfitting\n",
      "65\t637\t217\toverfitting\n",
      "65\t642\t426\toverfitting\n",
      "65\t644\t62\toverfitting\n",
      "77\t16\t346\toverfitting\n",
      "81\t38\t319\toverfitting\n",
      "81\t38\t392\toverfitting\n",
      "86\t614\t344\toverfitting\n",
      "88\t34\t388\toverfitting\n",
      "88\t34\t725\toverfitting\n",
      "90\t9\t177\toverfitting\n",
      "98\t4\t90\toverfitting\n",
      "103\t1564\t49\toverfitting\n",
      "110\t739\t65\toverfitting\n",
      "127\t51\t722\toverfit\n",
      "128\t4\t89\toverfitting\n",
      "139\t2\t1050\toverfitting\n",
      "146\t674\t35\toverfit\n",
      "146\t674\t54\toverfit\n",
      "158\t0\t15\toverfitting\n",
      "158\t0\t221\toverfitted\n",
      "158\t0\t341\toverfitting\n",
      "158\t0\t606\tunderfitted\n",
      "158\t1\t16\tunderfitting\n",
      "158\t2\t19\toverfitting\n",
      "158\t2\t359\toverfitting\n",
      "158\t4\t18\toverfitting\n",
      "158\t5\t39\toverfitting\n",
      "158\t7\t192\toverfitting\n",
      "158\t8\t355\toverfitted\n",
      "158\t8\t451\tunderfitting\n",
      "158\t8\t468\toverfitting\n",
      "158\t10\t138\toverfitting\n",
      "158\t13\t24\toverfitting\n",
      "158\t13\t681\toverfit\n",
      "158\t14\t258\toverfitting\n",
      "158\t17\t988\toverfits\n",
      "158\t17\t1024\toverfitted\n",
      "158\t17\t1896\toverfitting\n",
      "158\t19\t43\toverfit\n",
      "158\t19\t219\toverfitting\n",
      "158\t22\t32\toverfitting\n",
      "158\t23\t19\toverfitted\n",
      "158\t24\t16\toverfitted\n",
      "158\t29\t313\toverfitting\n",
      "158\t30\t5\tunderfitted\n",
      "158\t34\t354\toverfitting\n",
      "158\t40\t9\toverfitting\n",
      "176\t1144\t73\toverfitting\n",
      "178\t0\t389\toverfitting\n",
      "178\t2\t761\toverfitting\n",
      "178\t23\t509\toverfit\n",
      "199\t697\t69\toverfitting\n",
      "199\t697\t336\toverfitting\n",
      "199\t698\t309\toverfitting\n",
      "199\t744\t89\toverfitting\n",
      "202\t0\t211\toverfitting\n",
      "207\t3\t514\toverfitting\n",
      "207\t4\t242\toverfitting\n",
      "207\t6\t225\toverfit\n",
      "207\t8\t229\toverfitting\n",
      "207\t8\t367\toverfitting\n",
      "207\t12\t18\toverfitting\n",
      "207\t14\t656\toverfitting\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('(over|under)fit\\w*')\n",
    "\n",
    "for text_nr in range(len(corpus_texts)):\n",
    "    text = corpus_texts[text_nr]\n",
    "    lines = re.split('[\\n\\r]+',text)\n",
    "    for line_nr in range(len(lines)):\n",
    "        line = lines[line_nr]\n",
    "        start = 0\n",
    "        while start < len(line)-7: \n",
    "            match = pattern.search(line,start)\n",
    "            if match:\n",
    "                print(text_nr,line_nr,match.start(), match.group(0),sep='\\t')\n",
    "                start = match.end()\n",
    "            else:\n",
    "                start = len(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of printing we can also count the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('overfitting', 72), ('overfit', 9), ('overfitted', 6), ('underfitting', 3), ('underfitted', 2), ('underfit', 1), ('overfits', 1)]\n",
      "[('over', 88), ('under', 6)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "w_freq = Counter()\n",
    "pref_freq = Counter()\n",
    "\n",
    "for text_nr in range(len(corpus_texts)):\n",
    "    text = corpus_texts[text_nr]\n",
    "    lines = re.split('[\\n\\r]+',text)\n",
    "    for line_nr in range(len(lines)):\n",
    "        line = lines[line_nr]\n",
    "        start = 0\n",
    "        while start < len(line)-7: \n",
    "            match = pattern.search(line,start)\n",
    "            if match:\n",
    "                w_freq.update([match.group(0)])\n",
    "                pref_freq.update([match.group(1)])\n",
    "                start = match.end()\n",
    "            else:\n",
    "                start = len(line)\n",
    "                \n",
    "print(w_freq.most_common())\n",
    "print(pref_freq.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A second example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('example', 5), ('problem', 2), ('algorithm', 2), ('function', 2), ('and', 2)]\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('(a|an|the) (difficult|easy|simple) (\\w+)')\n",
    "\n",
    "w_freq = Counter()\n",
    "\n",
    "for text_nr in range(len(corpus_texts)):\n",
    "    text = corpus_texts[text_nr]\n",
    "    lines = re.split('[\\n\\r]+',text)\n",
    "    for line_nr in range(len(lines)):\n",
    "        line = lines[line_nr]\n",
    "        start = 0\n",
    "        while start < len(line)-7: \n",
    "            match = pattern.search(line,start)\n",
    "            if match:\n",
    "                w_freq.update([match.group(3)])\n",
    "                start = match.end()\n",
    "            else:\n",
    "                start = len(line)\n",
    "                \n",
    "print(w_freq.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
