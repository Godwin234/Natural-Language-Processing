{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Word Embeddings, e.g. for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some general modules\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# set logging level (suggested: logging.INFO; for bug fixing: logging.DEBUG)\n",
    "# logging_level = logging.INFO\n",
    "logging_level = logging.DEBUG\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging_level)\n",
    "\n",
    "RANDOM_SEED = 3756\n",
    "\n",
    "# set size of Word Embeddings\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data \n",
    "\n",
    "We will use the course corpus (see Learnweb) for these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    return json.loads(gzip.GzipFile(filename).read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus format:\n",
    "\n",
    "\t{\n",
    "\t'corpus creator 1' (string) : texts (list of strings),\n",
    "\t'corpus creator 2' (string) : texts (list of strings),\n",
    "\t...\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! you might have to change the path here !!!\n",
    "\n",
    "# corpus_data_file_path = Path('.') / 'nlpcm_corpus_1.json.gz'\n",
    "corpus_data_file_path = 'nlpcm_corpus_1.json.gz'\n",
    "\n",
    "course_corpus = load_data(corpus_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: The course corpus consists of 18 subcorpora:\n",
      "INFO: 0:   7517 texts gathered by Alexandre Carey with 4802229 characters in total.\n",
      "INFO: 1:   6585 texts gathered by Eunju Park with 4999802 characters in total.\n",
      "INFO: 2:    561 texts gathered by Anthony Angrimson with 1500862 characters in total.\n",
      "INFO: 3:   8241 texts gathered by Paweena Tarepakdee with 4999840 characters in total.\n",
      "INFO: 4:  11909 texts gathered by Olalekan Olayemi with 4999945 characters in total.\n",
      "INFO: 5:   5736 texts gathered by Godwin Ezeani with 4266978 characters in total.\n",
      "INFO: 6:  15368 texts gathered by Atulya Praphul with 4999981 characters in total.\n",
      "INFO: 7:     27 texts gathered by Muhammad Mehmood Ali with 1276567 characters in total.\n",
      "INFO: 8:   3921 texts gathered by Haolong Yan with 3942391 characters in total.\n",
      "INFO: 9:   1041 texts gathered by Patrick Schedlbauer with 4999721 characters in total.\n",
      "INFO: 10:   9305 texts gathered by Luis Diego Rosello Cordero with 4999833 characters in total.\n",
      "INFO: 11:   3500 texts gathered by Tanvi Vishwas Joglekar with 4999979 characters in total.\n",
      "INFO: 12:   3781 texts gathered by Shri Shalini Sekar with 4999809 characters in total.\n",
      "INFO: 13:    483 texts gathered by Philip Kurzend√∂rfer with 3540487 characters in total.\n",
      "INFO: 14:    660 texts gathered by Helle Hannken-Illjes with 3345541 characters in total.\n",
      "INFO: 15:   5355 texts gathered by Marcel Ritzmann with 3415194 characters in total.\n",
      "INFO: 16:    224 texts gathered by Istiaque Mannafee Shaikat with 4999971 characters in total.\n",
      "INFO: 17:   1417 texts gathered by Nourhan Ahmed with 2794273 characters in total.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.info(' The course corpus consists of %d subcorpora:' % len(course_corpus))\n",
    "index = 0\n",
    "for designer, texts in course_corpus.items():\n",
    "    logging.info(' %d: %6d texts gathered by %s with %d characters in total.' % (index, len(texts), designer, sum([len(text) for text in texts])))\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text of subcorpus 0 of 18\n",
      "Tokenizing text of subcorpus 1 of 18\n",
      "Tokenizing text of subcorpus 2 of 18\n",
      "Tokenizing text of subcorpus 3 of 18\n",
      "Tokenizing text of subcorpus 4 of 18\n",
      "Tokenizing text of subcorpus 5 of 18\n",
      "Tokenizing text of subcorpus 6 of 18\n",
      "Tokenizing text of subcorpus 7 of 18\n",
      "Tokenizing text of subcorpus 8 of 18\n",
      "Tokenizing text of subcorpus 9 of 18\n",
      "Tokenizing text of subcorpus 10 of 18\n",
      "Tokenizing text of subcorpus 11 of 18\n",
      "Tokenizing text of subcorpus 12 of 18\n",
      "Tokenizing text of subcorpus 13 of 18\n",
      "Tokenizing text of subcorpus 14 of 18\n",
      "Tokenizing text of subcorpus 15 of 18\n",
      "Tokenizing text of subcorpus 16 of 18\n",
      "Tokenizing text of subcorpus 17 of 18\n"
     ]
    }
   ],
   "source": [
    "# tokenize the entire corpus (this may take a few minutes!)\n",
    "import nltk\n",
    "\n",
    "sentences = []\n",
    "index = 0\n",
    "for designer, texts in course_corpus.items():\n",
    "    print('Tokenizing text of subcorpus %d of %d' % (index, len(course_corpus)))\n",
    "    index += 1\n",
    "    for text in texts:\n",
    "        for sentence in nltk.sent_tokenize(text, language=\"english\"):\n",
    "            tokenized_sentence = nltk.word_tokenize(sentence, language=\"english\")\n",
    "            sentences.append(tokenized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus number of sentences: 658902\n",
      "Corpus number of tokens: 14548903\n",
      "The first 5 sentences\n",
      "['I', 'love', 'all', 'of', 'the', 'new', 'editions', 'to', 'the', 'game', '.']\n",
      "['I', 'love', 'that', 'it', 'finally', 'FEELS', 'like', 'pokemon', '.']\n",
      "['The', 'problem', 'is', 'that', 'it', \"'s\", 'far', 'too', 'late', '.']\n",
      "['The', 'fact', 'that', 'you', 'guys', 'were', 'so', 'eager', 'to', 'release', 'this', 'game', 'before', 'it', 'was', 'finished', ',', 'before', 'you', 'had', 'the', 'server', 'capacity', '(', 'that', \"'s\", 'not', 'totally', 'your', 'fault', 'though', 'this', 'game', 'initially', 'blew', 'up', 'more', 'than', 'anyone', 'could', 'have', 'thought', ')', ',', 'there', 'was', 'no', 'features', 'to', 'battle', ',', 'interact', 'with', 'friends', ',', 'no', 'raids', 'or', 'gym', 'battles', ',', 'the', 'radar', 'had', 'constant', 'problems', ',', 'you', 'guys', 'would', \"n't\", 'allow', 'anyone', 'to', 'use', 'radars', 'because', 'your', 'guys', \"'\", 'app', 'kind', 'of', 'sucked', 'at', 'the', 'time', '.']\n",
      "['You', 'guys', 'just', 'handled', 'this', 'game', 'extremely', 'poorly', 'during', 'the', 'initial', 'first', '6', 'months', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Corpus number of sentences: %d' % len(sentences))\n",
    "print('Corpus number of tokens: %d' % sum([len(sentence) for sentence in sentences]))\n",
    "\n",
    "num_sent = 5\n",
    "print('The first %d sentences' % num_sent)\n",
    "for sentence in sentences[:num_sent]:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Word Embeddings\n",
    "\n",
    "We need a word index, a dictionary which maps every word to a unique integer value (also with two special values for padding and unknown words: <PAD\\> and <UNK\\>).\n",
    "Also we need an embedding_matrix, a numpy matrix which stores for each word from the word index a word embedding.\n",
    "\n",
    "### Suggestion: Use pre-trained word embeddings trained on a large corpus\n",
    "\n",
    "Some pre-trained word embeddings can be found here:<br>\n",
    " https://radimrehurek.com/gensim/models/keyedvectors.html<br>\n",
    " https://fasttext.cc/docs/en/english-vectors.html<br>\n",
    " https://nlp.stanford.edu/projects/glove/<br>\n",
    " https://www.spinningbytes.com/resources/wordembeddings/<br>\n",
    " https://code.google.com/archive/p/word2vec/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:loading projection weights from GoogleNews-vectors-negative300.bin.gz\n",
      "DEBUG:{'transport_params': None, 'ignore_ext': False, 'opener': None, 'closefd': True, 'newline': None, 'errors': None, 'encoding': None, 'buffering': -1, 'mode': 'rb', 'uri': 'GoogleNews-vectors-negative300.bin.gz'}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-31ab9936d6e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mword2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnb_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1547\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1549\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[0mbinary_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_TO_BINARY_LUT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mignore_ext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mdecompressed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[1;34m(uri, mode, transport_params)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[0mscheme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sniff_scheme\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[0msubmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m     \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TODO'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\local_file.py\u001b[0m in \u001b[0;36mopen_uri\u001b[1;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mparsed_uri\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uri_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "# for example, if you downloaded https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "nb_words = len(word2ind)+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word2ind.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Alternatively: Compute Word2Vec embeddings yourself using corpus data\n",
    " Here I will use the Brown corpus. Please note that these embeddings probably might not work well for our data as the Brown corpus is relatively old and small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load library gensim (contains word2vec implementation)\n",
    "import gensim\n",
    "\n",
    "# ignore some warnings (probably caused by gensim version)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores\n",
    "\n",
    "# load corpus data\n",
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(min_count=20,\n",
    "                                   window=2,\n",
    "                                   size=300,\n",
    "                                   sample=6e-5, \n",
    "                                   alpha=0.03, \n",
    "                                   min_alpha=0.0007, \n",
    "                                   negative=20,\n",
    "                                   workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:collecting all words and their counts\n",
      "INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
      "INFO:PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
      "INFO:PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
      "INFO:PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
      "INFO:PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
      "INFO:collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
      "INFO:Loading a fresh vocabulary\n",
      "INFO:effective_min_count=20 retains 5164 unique words (9% of original 56057, drops 50893)\n",
      "INFO:effective_min_count=20 leaves 1003451 word corpus (86% of original 1161192, drops 157741)\n",
      "INFO:deleting the raw counts dictionary of 56057 items\n",
      "INFO:sample=6e-05 downsamples 646 most-common words\n",
      "INFO:downsampling leaves estimated 389561 word corpus (38.8% of prior 1003451)\n",
      "INFO:estimated required memory for 5164 words and 300 dimensions: 14975600 bytes\n",
      "INFO:resetting layer weights\n",
      "INFO:training model with 3 workers on 5164 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2\n",
      "INFO:EPOCH 1 - PROGRESS: at 16.11% examples, 68115 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 1 - PROGRESS: at 33.41% examples, 68908 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 1 - PROGRESS: at 47.04% examples, 65591 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 1 - PROGRESS: at 59.73% examples, 63777 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 1 - PROGRESS: at 78.69% examples, 64043 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 36 jobs\n",
      "DEBUG:worker exiting, processed 43 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:EPOCH 1 - PROGRESS: at 99.06% examples, 62454 words/s, in_qsize 0, out_qsize 3\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 1 : training on 1161192 raw words (389678 effective words) took 6.2s, 62702 effective words/s\n",
      "INFO:EPOCH 2 - PROGRESS: at 15.37% examples, 63140 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 2 - PROGRESS: at 34.02% examples, 68712 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 2 - PROGRESS: at 50.98% examples, 71222 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:EPOCH 2 - PROGRESS: at 68.78% examples, 73149 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 2 - PROGRESS: at 91.27% examples, 70875 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 37 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 2 : training on 1161192 raw words (389851 effective words) took 5.6s, 69805 effective words/s\n",
      "INFO:EPOCH 3 - PROGRESS: at 16.11% examples, 66468 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 3 - PROGRESS: at 34.78% examples, 70683 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 3 - PROGRESS: at 50.98% examples, 71243 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 3 - PROGRESS: at 67.44% examples, 72655 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 3 - PROGRESS: at 90.29% examples, 70608 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:worker exiting, processed 37 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 3 : training on 1161192 raw words (389237 effective words) took 5.6s, 69192 effective words/s\n",
      "INFO:EPOCH 4 - PROGRESS: at 16.11% examples, 65220 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 4 - PROGRESS: at 30.18% examples, 60108 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 4 - PROGRESS: at 43.73% examples, 59038 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 4 - PROGRESS: at 59.07% examples, 62059 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 4 - PROGRESS: at 77.47% examples, 62570 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:EPOCH 4 - PROGRESS: at 100.00% examples, 62543 words/s, in_qsize 0, out_qsize 1\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 4 : training on 1161192 raw words (389902 effective words) took 6.2s, 62514 effective words/s\n",
      "INFO:EPOCH 5 - PROGRESS: at 16.11% examples, 65871 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 5 - PROGRESS: at 34.02% examples, 69348 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 5 - PROGRESS: at 50.98% examples, 71070 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 5 - PROGRESS: at 66.39% examples, 71767 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 5 - PROGRESS: at 90.29% examples, 70489 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 5 : training on 1161192 raw words (390004 effective words) took 5.7s, 68996 effective words/s\n",
      "INFO:EPOCH 6 - PROGRESS: at 16.11% examples, 66300 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 6 - PROGRESS: at 33.41% examples, 68450 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 6 - PROGRESS: at 49.43% examples, 69826 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 6 - PROGRESS: at 63.18% examples, 69116 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 6 - PROGRESS: at 81.06% examples, 65797 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 6 - PROGRESS: at 95.75% examples, 61114 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 6 : training on 1161192 raw words (389615 effective words) took 6.5s, 59631 effective words/s\n",
      "INFO:EPOCH 7 - PROGRESS: at 11.48% examples, 47970 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 7 - PROGRESS: at 26.29% examples, 52693 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 7 - PROGRESS: at 39.99% examples, 54824 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 7 - PROGRESS: at 53.06% examples, 56108 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 7 - PROGRESS: at 66.39% examples, 57528 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 7 - PROGRESS: at 84.69% examples, 56239 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 7 : training on 1161192 raw words (390028 effective words) took 7.1s, 55321 effective words/s\n",
      "INFO:EPOCH 8 - PROGRESS: at 13.09% examples, 53057 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 8 - PROGRESS: at 27.87% examples, 55110 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 8 - PROGRESS: at 41.53% examples, 55743 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 8 - PROGRESS: at 53.06% examples, 55281 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 8 - PROGRESS: at 66.39% examples, 56676 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 8 - PROGRESS: at 85.91% examples, 55756 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 8 : training on 1161192 raw words (389642 effective words) took 7.1s, 54943 effective words/s\n",
      "INFO:EPOCH 9 - PROGRESS: at 10.57% examples, 44585 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 9 - PROGRESS: at 21.75% examples, 44935 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 9 - PROGRESS: at 35.48% examples, 48749 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 9 - PROGRESS: at 49.43% examples, 51930 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 9 - PROGRESS: at 62.56% examples, 54238 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 9 - PROGRESS: at 81.06% examples, 54339 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 37 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "DEBUG:worker exiting, processed 41 jobs\n",
      "INFO:EPOCH 9 - PROGRESS: at 99.80% examples, 53606 words/s, in_qsize 0, out_qsize 3\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 9 : training on 1161192 raw words (389479 effective words) took 7.3s, 53361 effective words/s\n",
      "INFO:EPOCH 10 - PROGRESS: at 13.09% examples, 54564 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 10 - PROGRESS: at 27.87% examples, 56007 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 10 - PROGRESS: at 41.53% examples, 56878 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 10 - PROGRESS: at 54.57% examples, 58300 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 10 - PROGRESS: at 67.44% examples, 58481 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 10 - PROGRESS: at 87.02% examples, 57392 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:EPOCH - 10 : training on 1161192 raw words (389982 effective words) took 6.9s, 56399 effective words/s\n",
      "INFO:EPOCH 11 - PROGRESS: at 10.57% examples, 42102 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 11 - PROGRESS: at 24.43% examples, 48021 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 11 - PROGRESS: at 35.48% examples, 47339 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 11 - PROGRESS: at 46.48% examples, 47160 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 11 - PROGRESS: at 58.31% examples, 49170 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 11 - PROGRESS: at 73.02% examples, 50358 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 11 - PROGRESS: at 91.27% examples, 50251 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 11 : training on 1161192 raw words (389180 effective words) took 7.8s, 50053 effective words/s\n",
      "INFO:EPOCH 12 - PROGRESS: at 12.30% examples, 50625 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 12 - PROGRESS: at 26.29% examples, 51973 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 12 - PROGRESS: at 39.31% examples, 52968 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 12 - PROGRESS: at 51.67% examples, 54012 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 12 - PROGRESS: at 63.90% examples, 54973 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 12 - PROGRESS: at 81.06% examples, 54188 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 12 - PROGRESS: at 98.03% examples, 53140 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 12 : training on 1161192 raw words (389604 effective words) took 7.3s, 53082 effective words/s\n",
      "DEBUG:worker exiting, processed 42 jobs\n",
      "DEBUG:worker exiting, processed 37 jobs\n",
      "INFO:EPOCH 13 - PROGRESS: at 12.30% examples, 49429 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 13 - PROGRESS: at 25.39% examples, 50202 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 13 - PROGRESS: at 37.00% examples, 49779 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 13 - PROGRESS: at 47.04% examples, 48360 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 13 - PROGRESS: at 56.89% examples, 48043 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 13 - PROGRESS: at 68.78% examples, 48312 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 13 - PROGRESS: at 87.02% examples, 48151 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 13 : training on 1161192 raw words (389369 effective words) took 8.1s, 47834 effective words/s\n",
      "INFO:EPOCH 14 - PROGRESS: at 11.48% examples, 48446 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 14 - PROGRESS: at 25.39% examples, 50891 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 14 - PROGRESS: at 38.57% examples, 52289 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 14 - PROGRESS: at 50.98% examples, 53235 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 14 - PROGRESS: at 63.18% examples, 54203 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 14 - PROGRESS: at 79.88% examples, 53602 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 14 - PROGRESS: at 98.03% examples, 52794 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 37 jobs\n",
      "DEBUG:worker exiting, processed 41 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 14 : training on 1161192 raw words (389449 effective words) took 7.4s, 52360 effective words/s\n",
      "INFO:EPOCH 15 - PROGRESS: at 12.30% examples, 49753 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 15 - PROGRESS: at 25.39% examples, 51175 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 15 - PROGRESS: at 38.57% examples, 52121 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 15 - PROGRESS: at 48.67% examples, 50442 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 15 - PROGRESS: at 59.07% examples, 49801 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 15 - PROGRESS: at 71.94% examples, 49807 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 15 - PROGRESS: at 89.23% examples, 49298 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 15 : training on 1161192 raw words (389481 effective words) took 8.0s, 48914 effective words/s\n",
      "INFO:EPOCH 16 - PROGRESS: at 11.48% examples, 47718 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 16 - PROGRESS: at 24.43% examples, 48995 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 16 - PROGRESS: at 37.00% examples, 49952 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 16 - PROGRESS: at 49.43% examples, 50688 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 16 - PROGRESS: at 61.21% examples, 51968 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 16 - PROGRESS: at 76.21% examples, 51830 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:EPOCH 16 - PROGRESS: at 93.29% examples, 51077 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 16 : training on 1161192 raw words (389554 effective words) took 7.7s, 50556 effective words/s\n",
      "INFO:EPOCH 17 - PROGRESS: at 11.48% examples, 47507 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:EPOCH 17 - PROGRESS: at 24.43% examples, 49346 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 17 - PROGRESS: at 37.81% examples, 51109 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 17 - PROGRESS: at 48.67% examples, 50114 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 17 - PROGRESS: at 58.31% examples, 48906 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 17 - PROGRESS: at 69.81% examples, 48395 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 17 - PROGRESS: at 83.48% examples, 46961 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 17 - PROGRESS: at 98.86% examples, 46602 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 17 : training on 1161192 raw words (388920 effective words) took 8.4s, 46215 effective words/s\n",
      "INFO:EPOCH 18 - PROGRESS: at 11.48% examples, 46496 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 18 - PROGRESS: at 23.64% examples, 47341 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 18 - PROGRESS: at 35.48% examples, 48253 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 18 - PROGRESS: at 47.04% examples, 48837 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 18 - PROGRESS: at 58.31% examples, 49918 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 18 - PROGRESS: at 71.94% examples, 50457 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 18 - PROGRESS: at 88.13% examples, 49563 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 18 : training on 1161192 raw words (389961 effective words) took 8.0s, 48944 effective words/s\n",
      "INFO:EPOCH 19 - PROGRESS: at 11.48% examples, 46392 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 19 - PROGRESS: at 24.43% examples, 48086 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 19 - PROGRESS: at 37.00% examples, 48957 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 19 - PROGRESS: at 46.48% examples, 47063 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 19 - PROGRESS: at 55.32% examples, 46392 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 19 - PROGRESS: at 65.57% examples, 46758 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 19 - PROGRESS: at 81.06% examples, 46318 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 19 - PROGRESS: at 98.03% examples, 46156 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 19 : training on 1161192 raw words (389002 effective words) took 8.5s, 45878 effective words/s\n",
      "INFO:EPOCH 20 - PROGRESS: at 11.48% examples, 47843 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 20 - PROGRESS: at 24.43% examples, 48960 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 20 - PROGRESS: at 37.00% examples, 49937 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 20 - PROGRESS: at 48.67% examples, 49855 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 20 - PROGRESS: at 60.42% examples, 51178 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 20 - PROGRESS: at 73.93% examples, 50624 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 20 - PROGRESS: at 90.29% examples, 49691 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 20 : training on 1161192 raw words (389931 effective words) took 8.0s, 48944 effective words/s\n",
      "INFO:EPOCH 21 - PROGRESS: at 10.57% examples, 43045 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 21 - PROGRESS: at 22.82% examples, 44835 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 21 - PROGRESS: at 33.41% examples, 44306 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 21 - PROGRESS: at 42.29% examples, 42390 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 21 - PROGRESS: at 50.98% examples, 41935 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 21 - PROGRESS: at 61.21% examples, 43138 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 21 - PROGRESS: at 75.14% examples, 43487 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 21 - PROGRESS: at 91.27% examples, 43294 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 21 : training on 1161192 raw words (389475 effective words) took 9.1s, 42910 effective words/s\n",
      "INFO:EPOCH 22 - PROGRESS: at 10.57% examples, 44049 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 22 - PROGRESS: at 22.82% examples, 46322 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 22 - PROGRESS: at 34.78% examples, 47147 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 22 - PROGRESS: at 45.87% examples, 47373 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 22 - PROGRESS: at 56.89% examples, 48327 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 22 - PROGRESS: at 68.78% examples, 48938 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 22 - PROGRESS: at 84.69% examples, 48011 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:EPOCH 22 - PROGRESS: at 99.06% examples, 46913 words/s, in_qsize 0, out_qsize 3\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 22 : training on 1161192 raw words (390241 effective words) took 8.3s, 46989 effective words/s\n",
      "INFO:EPOCH 23 - PROGRESS: at 10.57% examples, 43333 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 23 - PROGRESS: at 19.95% examples, 41150 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 23 - PROGRESS: at 30.18% examples, 40110 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 23 - PROGRESS: at 39.31% examples, 39786 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 23 - PROGRESS: at 49.43% examples, 40716 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 23 - PROGRESS: at 60.42% examples, 42663 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 23 - PROGRESS: at 73.93% examples, 43195 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 23 - PROGRESS: at 90.29% examples, 43071 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:EPOCH - 23 : training on 1161192 raw words (389357 effective words) took 9.1s, 42894 effective words/s\n",
      "INFO:EPOCH 24 - PROGRESS: at 9.64% examples, 41103 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 24 - PROGRESS: at 20.81% examples, 43418 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 24 - PROGRESS: at 32.60% examples, 43933 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 24 - PROGRESS: at 43.73% examples, 44918 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 24 - PROGRESS: at 54.57% examples, 46450 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 24 - PROGRESS: at 66.39% examples, 47519 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 24 - PROGRESS: at 82.18% examples, 47048 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 24 - PROGRESS: at 98.03% examples, 46502 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 24 : training on 1161192 raw words (389614 effective words) took 8.4s, 46132 effective words/s\n",
      "INFO:EPOCH 25 - PROGRESS: at 8.01% examples, 33278 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 25 - PROGRESS: at 17.44% examples, 35561 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 25 - PROGRESS: at 28.72% examples, 38253 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 25 - PROGRESS: at 39.31% examples, 40101 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 25 - PROGRESS: at 50.98% examples, 42211 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 25 - PROGRESS: at 62.56% examples, 44532 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 25 - PROGRESS: at 83.48% examples, 47161 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 25 : training on 1161192 raw words (389498 effective words) took 8.1s, 47871 effective words/s\n",
      "INFO:EPOCH 26 - PROGRESS: at 15.37% examples, 62489 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 26 - PROGRESS: at 31.82% examples, 64555 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 26 - PROGRESS: at 47.04% examples, 65197 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 26 - PROGRESS: at 61.85% examples, 66630 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 26 - PROGRESS: at 81.06% examples, 65574 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "DEBUG:worker exiting, processed 41 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 26 : training on 1161192 raw words (389497 effective words) took 6.1s, 64222 effective words/s\n",
      "INFO:EPOCH 27 - PROGRESS: at 14.64% examples, 61467 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 27 - PROGRESS: at 28.72% examples, 57567 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 27 - PROGRESS: at 39.99% examples, 54419 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 27 - PROGRESS: at 53.82% examples, 56952 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 27 - PROGRESS: at 66.39% examples, 57212 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 27 - PROGRESS: at 88.13% examples, 57632 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 27 : training on 1161192 raw words (389653 effective words) took 6.8s, 57007 effective words/s\n",
      "INFO:EPOCH 28 - PROGRESS: at 14.64% examples, 59784 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 28 - PROGRESS: at 30.18% examples, 61701 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 28 - PROGRESS: at 45.87% examples, 63104 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 28 - PROGRESS: at 61.21% examples, 65638 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 28 - PROGRESS: at 81.06% examples, 65330 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 40 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 28 : training on 1161192 raw words (389171 effective words) took 6.1s, 63856 effective words/s\n",
      "INFO:EPOCH 29 - PROGRESS: at 12.30% examples, 48808 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 29 - PROGRESS: at 25.39% examples, 50461 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 29 - PROGRESS: at 37.81% examples, 50759 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 29 - PROGRESS: at 50.98% examples, 52761 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 29 - PROGRESS: at 62.56% examples, 53691 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 29 - PROGRESS: at 77.47% examples, 52549 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 29 - PROGRESS: at 94.58% examples, 51695 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 41 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 37 jobs\n",
      "DEBUG:worker exiting, processed 39 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 29 : training on 1161192 raw words (389374 effective words) took 7.6s, 50978 effective words/s\n",
      "INFO:EPOCH 30 - PROGRESS: at 11.48% examples, 47629 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 30 - PROGRESS: at 27.01% examples, 55473 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 30 - PROGRESS: at 42.97% examples, 59457 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:EPOCH 30 - PROGRESS: at 57.63% examples, 61809 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 30 - PROGRESS: at 75.14% examples, 62600 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:EPOCH 30 - PROGRESS: at 96.96% examples, 62136 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:job loop exiting, total 117 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:worker exiting, processed 41 jobs\n",
      "DEBUG:worker exiting, processed 38 jobs\n",
      "INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:EPOCH - 30 : training on 1161192 raw words (389688 effective words) took 6.3s, 61533 effective words/s\n",
      "INFO:training on a 34835760 raw words (11687437 effective words) took 219.8s, 53180 effective words/s\n",
      "INFO:precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(brown.sents(), progress_per=10000)\n",
    "w2v_model.train(brown.sents(), total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "embedding_matrix = np.random.uniform(-1, 1, (len(word_index) + len(w2v_model.wv.vocab), EMBEDDING_DIM))\n",
    "for word in w2v_model.wv.vocab:\n",
    "    index = len(word_index)\n",
    "    word_index[word] = index\n",
    "    embedding_matrix[index] = w2v_model.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classification task\n",
    "\n",
    "sentence_min_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO:Processing texts of designer Alexandre Carey; 18 instances so far.\n",
      "INFO:Processing texts of designer Eunju Park; 18 instances so far.\n",
      "INFO:Processing texts of designer Anthony Angrimson; 18 instances so far.\n",
      "INFO:Processing texts of designer Paweena Tarepakdee; 18 instances so far.\n",
      "INFO:Processing texts of designer Olalekan Olayemi; 18 instances so far.\n",
      "INFO:Processing texts of designer Godwin Ezeani; 18 instances so far.\n",
      "INFO:Processing texts of designer Atulya Praphul; 18 instances so far.\n",
      "INFO:Processing texts of designer Muhammad Mehmood Ali; 18 instances so far.\n",
      "INFO:Processing texts of designer Haolong Yan; 18 instances so far.\n",
      "INFO:Processing texts of designer Patrick Schedlbauer; 18 instances so far.\n",
      "INFO:Processing texts of designer Luis Diego Rosello Cordero; 18 instances so far.\n",
      "INFO:Processing texts of designer Tanvi Vishwas Joglekar; 18 instances so far.\n",
      "INFO:Processing texts of designer Shri Shalini Sekar; 18 instances so far.\n",
      "INFO:Processing texts of designer Philip Kurzend√∂rfer; 18 instances so far.\n",
      "INFO:Processing texts of designer Helle Hannken-Illjes; 18 instances so far.\n",
      "INFO:Processing texts of designer Marcel Ritzmann; 18 instances so far.\n",
      "INFO:Processing texts of designer Istiaque Mannafee Shaikat; 18 instances so far.\n",
      "INFO:Processing texts of designer Nourhan Ahmed; 18 instances so far.\n"
     ]
    }
   ],
   "source": [
    "# prepare the data: we first need to create integer vectors using our word index to represent words as numerical values for the input layer of our network.\n",
    "\n",
    "# We also have to tokenize the data:\n",
    "# To be precise, we might want to use the exact same tokenizer which was used for preparing the data for training the word embeddings!\n",
    "# Here, the NLTK-tokenizer is used. NOTE: this might take a few minutes!\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "instances = {designer: [] for designer in course_corpus}\n",
    "for designer, texts in course_corpus.items():\n",
    "    logging.info('Processing texts of designer %s; %d instances so far.' % (designer, len(instances)))\n",
    "    for text in texts:\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            if len(tokens) >= sentence_min_length:\n",
    "                instances[designer].append([word_index.get(word, word_index[\"<UNK>\"]) for word in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Train data instances: 289913.\n",
      "INFO:Test data instances: 32214.\n"
     ]
    }
   ],
   "source": [
    "# split into training and test data instances\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "TEST_TRAIN_RATIO = 0.1\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "def pad_input(sentences):\n",
    "    return keras.preprocessing.sequence.pad_sequences(\n",
    "        sentences, maxlen=MAX_SEQUENCE_LENGTH, dtype='int32', padding='pre', truncating='pre', value=word_index[\"<PAD>\"])\n",
    "\n",
    "train_labeled_data = []\n",
    "test_labeled_data = []\n",
    "designer_index = {}\n",
    "for designer, designer_instances in instances.items():\n",
    "    designer_index[designer] = len(designer_index)\n",
    "    random.shuffle(designer_instances)\n",
    "    test_labeled_data += [(inst, designer_index[designer]) for inst in designer_instances[:round(len(designer_instances)*TEST_TRAIN_RATIO)]]\n",
    "    train_labeled_data += [(inst, designer_index[designer]) for inst in designer_instances[round(len(designer_instances)*TEST_TRAIN_RATIO):]]\n",
    "\n",
    "random.shuffle(train_labeled_data)\n",
    "train_data = pad_input([inst[0] for inst in train_labeled_data])\n",
    "train_labels = [inst[1] for inst in train_labeled_data]\n",
    "\n",
    "random.shuffle(test_labeled_data)\n",
    "test_data = pad_input([inst[0] for inst in test_labeled_data])\n",
    "test_labels = [inst[1] for inst in test_labeled_data]\n",
    "#test_data = np.array(test_data)\n",
    "#test_labels = np.array(test_labels)\n",
    "logging.info('Train data instances: %d.' % len(train_data))\n",
    "logging.info('Test data instances: %d.' % len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(max(train_labels))\n",
    "print(min(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.compat.v1.Session() as sesh:\n",
    "#    train_labels = sesh.run(tf.one_hot(train_labels, 18))\n",
    "    \n",
    "    #y_val = sesh.run(tf.one_hot(y_val, 28))\n",
    " #   test_labels = sesh.run(tf.one_hot(test_labels,18))\n",
    "#test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(40,)))\n",
    "model.add(Embedding(len(word_index),\n",
    "                                         EMBEDDING_DIM,\n",
    "                                         weights=[embedding_matrix],\n",
    "                                         input_length=40,\n",
    "                                         trainable=False))\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=34, kernel_size=2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dense(len(course_corpus),\n",
    "                                  activation='softmax'))\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 11s 374ms/step - loss: 1.0174 - accuracy: 0.6757 - val_loss: 1.3081 - val_accuracy: 0.6000\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 12s 402ms/step - loss: 1.0166 - accuracy: 0.6729 - val_loss: 1.2954 - val_accuracy: 0.6000\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 12s 389ms/step - loss: 1.0122 - accuracy: 0.6760 - val_loss: 1.2949 - val_accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 11s 353ms/step - loss: 1.0075 - accuracy: 0.6784 - val_loss: 1.2935 - val_accuracy: 0.5980\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 11s 382ms/step - loss: 0.9988 - accuracy: 0.6782 - val_loss: 1.2828 - val_accuracy: 0.6030\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 11s 381ms/step - loss: 1.0003 - accuracy: 0.6820 - val_loss: 1.2790 - val_accuracy: 0.5960\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 12s 387ms/step - loss: 0.9972 - accuracy: 0.6792 - val_loss: 1.2699 - val_accuracy: 0.5990\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 11s 379ms/step - loss: 0.9952 - accuracy: 0.6823 - val_loss: 1.2791 - val_accuracy: 0.6060\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 12s 385ms/step - loss: 0.9921 - accuracy: 0.6825 - val_loss: 1.2709 - val_accuracy: 0.6020\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 11s 380ms/step - loss: 0.9933 - accuracy: 0.6825 - val_loss: 1.2793 - val_accuracy: 0.6010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x256bc559d68>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_limit = 30000\n",
    "model.fit(np.array(train_data[:train_limit]),\n",
    "          np.array(train_labels[:train_limit]),\n",
    "          epochs=10,\n",
    "          batch_size = 1012,\n",
    "          verbose=1,\n",
    "          validation_data=(np.array(test_data[:test_limit]), np.array(test_labels[:test_limit])))\n",
    "#_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 1.2238 - accuracy: 0.6139: 0s - los\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.22384512424469, 0.6139000058174133]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "test_limit = 10000\n",
    "model.evaluate(np.array(test_data[:test_limit]), np.array(test_labels[:test_limit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 12, 8, 4, 3, 10, 16, 12, 17]\n",
      "[5, 1, 12, 8, 8, 3, 10, 13, 12, 17]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:10])\n",
    "print([max([(ind, v) for ind, v in enumerate(pred)], key=lambda p: p[1])[0] for pred in model.predict(test_data[:10])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
